{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref# https://scikit-learn.org/stable/auto_examples/plot_anomaly_comparison.html#sphx-glr-auto-examples-plot-anomaly-comparison-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing anomaly detection algorithms for outlier detection on toy datasets\n",
    "\n",
    "\n",
    "This example shows characteristics of different anomaly detection algorithms\n",
    "on 2D datasets. Datasets contain one or two modes (regions of high density)\n",
    "to illustrate the ability of algorithms to cope with multimodal data.\n",
    "\n",
    "For each dataset, 15% of samples are generated as random uniform noise. \n",
    "Decision boundaries between inliers and outliers are displayed in black\n",
    "except for Local Outlier Factor (LOF) and Unsupervised Sparse Projection \n",
    "Oblique Randomer Forests (USPORF) as it has no predict method to be applied\n",
    "on new data when it is used for outlier detection. In the example, Extended Isolation Forest (EIF) and USPORF are not yet in the sklearn yet. This experiment uses adjusted rand index to compare the each outlier prediction performance.\n",
    "\n",
    "The [`sklearn.svm.OneClassSVM`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) is known to be sensitive to outliers and\n",
    "thus does not perform very well for outlier detection. This estimator is best\n",
    "suited for novelty detection when the training set is not contaminated by\n",
    "outliers. That said, outlier detection in high-dimension, or without any\n",
    "assumptions on the distribution of the inlying data is very challenging, and a\n",
    "One-class SVM might give useful results in these situations depending on the\n",
    "value of its hyperparameters.\n",
    "\n",
    "[`sklearn.covariance.EllipticEnvelope`](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope) assumes the data is Gaussian and\n",
    "learns an ellipse. It thus degrades when the data is not unimodal. Notice\n",
    "however that this estimator is robust to outliers.\n",
    "\n",
    "[`sklearn.ensemble.IsolationForest`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest), \n",
    "[`Extended Isolation Forest`](https://github.com/sahandha/eif/blob/master/eif.py),\n",
    "and [`sklearn.neighbors.LocalOutlierFactor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor) seem to perform reasonably well\n",
    "for multi-modal data sets. The advantage of\n",
    "[`sklearn.neighbors.LocalOutlierFactor`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor) over the other estimators is\n",
    "shown for the third data set, where the two modes have different densities.\n",
    "This advantage is explained by the local aspect of LOF, meaning that it only\n",
    "compares the score of abnormality of one sample with the scores of its\n",
    "neighbors.\n",
    "sdaasdasdadad\n",
    "\n",
    "[`USPORF`](https://github.com/neurodata/SPORF/blob/7c4cff4/Python/rerf/urerf.py#L12) seems to better suit novelty detection because it measure the sum roll of similarity matrix. The algorithm sometimes assigns a close group of outliers as inliers.)\n",
    "\n",
    "Finally, for the last data set, it is hard to say that one sample is more\n",
    "abnormal than another sample as they are uniformly distributed in a\n",
    "hypercube. Except for the [`sklearn.svm.OneClassSVM`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM) which overfits a\n",
    "little, all estimators present decent solutions for this situation. In such a\n",
    "case, it would be wise to look more closely at the scores of abnormality of\n",
    "the samples as a good estimator should assign similar scores to all the\n",
    "samples.\n",
    "\n",
    "While these examples give some intuition about the algorithms, this\n",
    "intuition might not apply to very high dimensional data.\n",
    "\n",
    "Finally, note that parameters of the models have been here handpicked but\n",
    "that in practice they need to be adjusted. In the absence of labelled data,\n",
    "the problem is completely unsupervised so model selection can be a challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#         Albert Thomas <albert.thomas@telecom-paristech.fr>\n",
    "# Editor: Pharuj Rajborirug (11/9/2019)\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from rerf.urerf import UnsupervisedRandomForest \n",
    "import eif as iso\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "matplotlib.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "\n",
    "# Example settings\n",
    "n_samples = 300\n",
    "outliers_fraction = 0.15\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "n_inliers = n_samples - n_outliers\n",
    "\n",
    "# Define datasets\n",
    "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "datasets = [\n",
    "    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,\n",
    "               **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],\n",
    "               **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],\n",
    "               **blobs_params)[0],\n",
    "    4. * (make_moons(n_samples=n_inliers, noise=.05, random_state=0)[0] -\n",
    "          np.array([0.5, 0.25])),\n",
    "    14. * (np.random.RandomState(42).rand(n_inliers, 2) - 0.5)]\n",
    "\n",
    "# Define to data label\n",
    "labels = np.concatenate([np.ones(n_inliers),-np.ones(n_outliers)], axis=0)\n",
    "\n",
    "# define outlier/anomaly detection methods to be compared\n",
    "anomaly_algorithms = [\n",
    "    (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction)),\n",
    "    (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\",\n",
    "                                      gamma=0.1)),\n",
    "    (\"Isolation Forest\", IsolationForest(behaviour='new',\n",
    "                                         contamination=outliers_fraction,\n",
    "                                         random_state=42)),\n",
    "    (\"Local Outlier Factor\", LocalOutlierFactor(\n",
    "        n_neighbors=35, contamination=outliers_fraction)),\n",
    "    (\"Extended IF\", iso.iForest(datasets[0], ntrees=100, sample_size=255, ExtensionLevel=1)),\n",
    "    (\"USPORF\",UnsupervisedRandomForest(feature_combinations='auto', max_depth=None,\n",
    "                         max_features='auto', min_samples_split='auto',\n",
    "                         n_estimators=100, n_jobs=None,\n",
    "                         projection_matrix='RerF'))] \n",
    "\n",
    "# Compare given classifiers under given settings\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 150),\n",
    "                     np.linspace(-7, 7, 150))\n",
    "\n",
    "plt.figure(figsize=(len(anomaly_algorithms) * 1.8 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "for i_dataset, X in enumerate(datasets):\n",
    "    # Add outliers (i = index (0,1,..), X = data set)\n",
    "    X = np.concatenate([X, rng.uniform(low=-6, high=6,\n",
    "                       size=(n_outliers, 2))], axis=0)                  # add uniform outlier\n",
    "    for name, algorithm in anomaly_algorithms: \n",
    "        # name =\"USPORF\", algorithm =  UnsupervisedRandomForest(_,_,_)\n",
    "        t0 = time.time()\n",
    "        if name == \"Extended IF\":                                       # Extended IF doesn't has fit function\n",
    "            algorithm = iso.iForest(X, ntrees=100, sample_size=min(256,len(X)), ExtensionLevel=1)\n",
    "        else:\n",
    "            algorithm.fit(X)\n",
    "        t1 = time.time()\n",
    "        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18) # function \"name\" to be title\n",
    "\n",
    "        # fit the data and tag outliers\n",
    "        if name == \"Local Outlier Factor\":\n",
    "            y_pred = algorithm.fit_predict(X)\n",
    "        elif name == \"Extended IF\":                                     # Extended IF doesn't have predict function\n",
    "            Score = algorithm.compute_paths(X_in=X)                     # compute anomaly score\n",
    "            sE = np.argsort(Score)\n",
    "            indicesE = sE[-int(np.ceil(outliers_fraction*X.shape[0])):] # outlier indices \n",
    "            y_pred = np.ones(X.shape[0])\n",
    "            y_pred[indicesE] = -1\n",
    "            y_pred = y_pred.astype(int)                                 # convert float to int array\n",
    "            \n",
    "            # Separation score \n",
    "            Score0= (Score[sE[-int(np.ceil(outliers_fraction*X.shape[0]))]]+Score[sE[-int(np.ceil(outliers_fraction*X.shape[0]))-1]])/2\n",
    "            \n",
    "        elif name == \"USPORF\":                                          # USPORF doesn't have predict function\n",
    "            sim_mat = algorithm.transform()                             # create similarity matrix\n",
    "            sim_sum = sim_mat.sum(axis=1)                               \n",
    "            sU = np.argsort(sim_sum)\n",
    "            indicesU = sU[:int(np.floor(outliers_fraction*X.shape[0]))] # outlier indeces\n",
    "            y_pred = np.ones(X.shape[0])\n",
    "            y_pred[indicesU] = -1\n",
    "            y_pred = y_pred.astype(int)                                 # convert float to int array\n",
    "        else:\n",
    "            y_pred = algorithm.fit(X).predict(X)\n",
    "            \n",
    "        #calculate ARI score\n",
    "        ARI = adjusted_rand_score(y_pred, labels)\n",
    "            \n",
    "        # plot the levels lines and the points\n",
    "        if name in (\"Robust covariance\",\"One-Class SVM\",\"Isolation Forest\"):\n",
    "            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "        if name == \"Extended IF\":\n",
    "            Z_Score = algorithm.compute_paths(X_in=np.c_[xx.ravel(), yy.ravel()]) \n",
    "            Z = np.ones(np.c_[xx.ravel(), yy.ravel()].shape[0])\n",
    "            for index, element in enumerate(Z_Score):                  # If the element in Z_score >= Score0, assign it to be an outlier\n",
    "                if element >= Score0:\n",
    "                    Z[index]=-1\n",
    "            Z = Z.astype(int)\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "            \n",
    "        colors = np.array(['#377eb8', '#ff7f00'])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[((y_pred + 1) // 2)]) # label color\n",
    "\n",
    "        plt.xlim(-7, 7)\n",
    "        plt.ylim(-7, 7)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')                         # Calculation time\n",
    "        plt.text(.01, .01, ('ARI %.3f' % ARI).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='left')                          # ARI score\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
