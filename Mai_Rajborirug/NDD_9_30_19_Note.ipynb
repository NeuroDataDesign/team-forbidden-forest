{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note Date 09/30/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "---\n",
    "\n",
    "### 1. Outliner Detection Methods\n",
    "source: [Novelty and Outlier Detection\n",
    ", sckitlearn](https://scikit-learn.org/stable/modules/outlier_detection.html)<br>\n",
    "source2: [A Brief Overview of Outlier Detection Techniques](https://towardsdatascience.com/a-brief-overview-of-outlier-detection-techniques-1e0b2c19e561), \n",
    "\n",
    "Outliers are the extreme values that diverge from overall pattern on a sample. These data can the learning algorithm less accurate. This data can consider as noise in high dimemsion space. This project focus on multivariate, non-parametric (n-features with unknown distribution) outliers.\n",
    "\n",
    "These are most popularmethods for outlier detection:\n",
    "* Z-score or Extreme Value Analysis (univeriate, parametric: Gaussian)\n",
    "* Probabilistic and Statistical Modeling (parametric)\n",
    "    - Determine unlikely instances from a probabilistic model of the data. For example, gaussian mixture models optimized using expectation-maximization.\n",
    "* Linear Regression Models (PCA, LMS)\n",
    "    - Least Means Square (LMS)\n",
    "    - Principal Component Analysis (PCA): a technique for feature extraction that combine input variable (dimension reduction) to drop the 'least important' variables. The drawback of the new combine variables are that they becomes less interpretable i.e. ($\\{X, Y\\} \\rightarrow \\{X+Y, X-Y\\}$ and drop $X-Y$)\n",
    "* Proximity Based Models (non-parametric)\n",
    "    - Identify the data that isolates from the mass as outliers, using clustering, density, nearest neighbor analysis\n",
    "* Information Theory Models\n",
    "* High Dimensional Outlier Detection Methods (high dimensional sparse data)\n",
    "\n",
    "---\n",
    "### 2. Isolation Forest\n",
    "Source: [IsolationForest, towards data science](https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e) <br>\n",
    "Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008, December). Isolation forest. In _2008 Eighth IEEE International Conference on Data Mining_ (pp. 413-422). IEEE.\n",
    "\n",
    "Types: multiverate, non-parametric, unsupervised learning\n",
    "#### Background\n",
    "- Most existing outlier detection construct a proﬁle of normal instances, then identify instances that do not conform to the normal proﬁle as anomalies\n",
    "- Isolation Forest explicitly isolate outliers instead of profile normal points.\n",
    "- Isolation Forest works well in high-D problems.\n",
    "- When generating a random tree, the algorithm recursively repeats the partitioning until all data are separate. The outliers will have a shorter path for two reasons.\n",
    "    - Fewer outliers leads to shorter paths.\n",
    "    - The extreme values of features from outliers are susceptable to be separate early.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1. Construct binary tree from: $T$ trees, $X=\\{x_1,x_2,..,x_n\\}$ data sample, $q$ feature, $p$ split values when $p>q$\n",
    "- Path length $h(x)$ is the number of edge measured from thr root node to the turminal node.\n",
    "- While the maximum possible height of iTree grows in the order of $n$, the average height grows in the order of $\\log n$\n",
    "- To detect the outliners $$c(n)=2H(n-1)-2(n-1)/n$$ \n",
    "$$s(x,n)=2^{-\\frac{E(h(X))}{c(n)}}$$. \n",
    "    - $H(i)$: harmonic number $\\ln (i) +0.5772$\n",
    "    - $c(n)$: average $h(x)$ given $n$\n",
    "    - $0< s\\leq 1$: the outlier will have short $h(X)$ and give $s(x,n)$ close to 1\n",
    "    \n",
    "---\n",
    "### 3. Gaussian Mixture Model (GMM)\n",
    "Source: [GaussianMixture, sckitlearn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)<br>[Mixture_model, wiki](https://en.wikipedia.org/wiki/Mixture_model)<br> [brilliant, gaussian-mixture-model](https://brilliant.org/wiki/gaussian-mixture-model/)\n",
    "\n",
    "Types: multiverate, parametric, unsupervised learning\n",
    "#### Background\n",
    "- GMM can solve the clustering problems when there is more than one peak (cluster)\n",
    "\n",
    "#### Algorithm\n",
    "1. Assign the distribution function for more than one means $$p(\\vec{x}) = \\sum^K_{i=1}\\phi_i \\mathcal{N}(\\vec{x}|\\vec{\\mu}_i,\\Sigma_i) $$, $$ \\mathcal{N}(\\vec{x}|\\vec{\\mu}_i,\\Sigma_i)=\\frac{1}{\\sqrt{(2\\pi)^K |\\Sigma_i|}}\\exp \\Big(-\\frac{1}{2}(\\vec{x}-\\vec{\\mu}_i)^T \\Sigma_i^{-1} (\\vec{x}-\\vec{\\mu}_i)\\Big)$$\n",
    "    - $K$ is number of cluster or '__expectation maximization (EM)__'\n",
    "    - Each cluster has mean $\\vec{\\mu_i}$\n",
    "    - The weight function $p(C_k)=\\phi_k$ has constrain $\\sum^K_{i=1} \\phi_i=1$\n",
    "    - $\\Sigma_i$ is covarient matrix\n",
    "    - sample variance $\\sigma_i^2$\n",
    "- Proceed expectation maximization (GM) for GMM\n",
    "    1. 'E' (expectation) step: using Bayesian's theorem to calculate $$\\hat{\\gamma}_{ik} =p(C_k|x_i,\\hat{\\phi},\\hat{\\mu},\\hat{\\sigma})= \\frac{\\hat{\\phi}_k \\mathcal{N}(x_i|\\hat{\\mu}_k,\\hat{\\sigma}_k)}{\\sum^K_{j=1} \\hat{\\phi}_j \\mathcal{N}(x_i|\\hat{\\mu}_k,\\hat{\\sigma}_k)}$$ for each data point $x_i \\in X$ given $\\phi_k, \\mu_k, \\sigma_k$\n",
    "    2. 'M' (maximization) step: use $\\hat{\\gamma}_{i,k}$ to update $\\phi_k, \\mu_k, \\sigma_k$ and repeat 'E-M' steps\n",
    "    3. The algorithm stop when the model does reach the goal: $\\forall$ parameter $\\theta_t$ at iteration $t|\\theta_t-\\theta_{t-1}|\\leq \\epsilon$, when $\\epsilon$ is some assigned number\n",
    "- There is trade off between fit and smaller $K$ (simpler model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
