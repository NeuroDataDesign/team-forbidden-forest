## 1.Intro:

Operating on the nearest neighbour might not always be desirable.
Measurement of error modelling – assuming that x’s are noisy data that are true but unobserved. Better to assume the noise than a noise free measurement. Since using kernel regression on noise-free measurement is useless it promotes to understand the latent structure of the data. Performance increases when only learning the structure of x explicit of the label y.
(Geodesic distance – shortest path considering a curved surface)
(Geodesic learning is removing this geodesic distance from data corpus (being only text information?))
Space partitioning trees uses binary and recursive splits with hyperplanes -> these are optimized for relative proximities of noisy measurements.
Decision trees are linked to kernel learning (what is kernel learning?)
When we’re talking about high dimensional data and the noise associated with it, what does it really mean with regards to the features/dataset?
URerF was developed for linear space and time complexity which approximates the latent geodesic distances between all pairs of points. 
Does not compute the geodesic distances between all points but instead looks at the latent structure and makes clusters in subspaces recursively.
Randomer forest-> enables URerF to separate meaningful data from noise.
Splitting criteria is introduced – Fast BIC -> computation of BIC for gaussian mixture model in ONE dimension.
(gaussian mixture model?)(Embedding?)
URerF find nearest neighbour in low dimensions spaces even amongst the noise better than most algorithms.

## 2.Related work:
Steps for preserving and estimation geodesic distances : 
1) Estimate geodesic distance in the original manifold 2)
All-pair shortest path is computed 
3)points are embedded into a lower dimensional space to preserve the distances.
UMAP – new algorithm for dimensionality reduction. The process is weighted k-nearest neighbors and embed into a lower dimension through force directed layout algo.
In comparison to random projection forest, the splits are optimized and uses a forest of many trees.

## 3.Unsupervised randomer forests:
Distinction b/w random forests – new fast BIC splitting criterion, “ramdomer” because the splitting methods are based on random sparse linear combinations of the features used to strengthen each tree.
Proximity matrix from random forests?

### Splitting criteria
Build T decision tree, split the parent node into child nodes recursively until a split criterion is met. Each node has ‘d’ features to search over, which is evaluated using splitting criterion. Using which it splits into daughter nodes.A is generated by randomly sampling from {−1, +1} λpd times, then distributing these values uniformly at random in A . The λ parameter is used to control the sparsity of A. d new features is now a sparse linear combination of the p original features 
Goal of splitting is to reduce intra-cluster variance
Sort the data, split between sequential points(ascending order)
Samples from left split point forms the left cluster and vice versa for the right cluster.
Two variable gausian mixture model is used for getting a global MLE than a local MLE(which is undersirable)
Proximity matrix contruction - based on similarity matrix. checks fraction of times the points i,j belonging to that given leaf node and across T tress occurs.
Computing geodesic precision and recall - average the geodesic precision and recall over each sample point.
(continous and discrete geodesics?)

## 4.Numerical results
Linear case - Euclidean distance recovers the geodesics with no noise.
Helix case - Euclidean performes poorly as the data corpus is designed to suit manifold learning algorithms, due to embedding of a 1D value in a 3D space?
Sphere case - performes better as the latent structure is in 2D which can be extended to a higher dimensional space.
Gausian mixture - most chanllenging case
### Choosing splitting criteria
Sample 1000 pts & cal geodesic precision recall
Looking at all the above cases - comparision were done in two domains : URF vs URerF and in conjuction with 3 splitting criterion - TwoMeans, BICMClust, BICFast.
The BICFast along with URerf outperfoms all other measures.
Other important parameters to consider - 
min- parent (cardinality of the smallest node)
mtry (number of features to test at each node)
--Geodesic precision using URerF is unaffected by changes in hyperparameters.
Other algorithms achieve a higher geodesic recall than URerF when there is no noise dimensions added, but degrade much more quickly than URerF upon the addition of noise dimensions.
## 5.Discussion
1) Proposed a geodesic distance learning method using URerF
2) Proposed new splitting criteria - Fast-BIC
3) Found URerF to be robust to noise dimensions
4) Other algorithms achieve a higher geodesic recall than URerF when there is no noise dimensions added, but degrade much more quickly than URerF upon the addition of noise dimensions.
